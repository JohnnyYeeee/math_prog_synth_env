[run]
name = unit-testing
seed = 0
gpu_id = 0

[env]
num_problems_per_module: 10 ** 5
validation_percentage: 0.2
max_sequence_length: 350
vocab_size: 250
max_difficulty: 0
univariate_differentiation: False
num_environments = 10
tokenizer_filepath = "artifacts/tokenizer.model"
max_formal_elements = 13
max_num_nodes = 10
trajectory_cache_filepath = "mathematics_dataset-v1.0/trajectory_cache.sqlite"

data_dirpath = "artifacts"
selected_filenames = ["short_problems.txt"]

# used only to initialize the tokenizer
types = [
         "EquationOrExpression",
         "Equation",
         "Function",
         "Expression",
         "Variable",
         "Value",
         "Rational"
         ]

operators = [
    "lookup_value",
    "solve_system",
    "append",
    "append_to_empty_list",
    "make_equation",
    "lookup_value_equation",
    "extract_isolated_variable",
    "substitution_left_to_right",
    "factor",
    "differentiate",
    "differentiate_wrt",
    "simplify",
    "make_function",
    "replace_arg",
    "mod",
    "gcd",
    "divides",
    "is_prime",
    "lcm",
    "lcd",
    "prime_factors",
    "evaluate_function",
    "not_op"
    ]

[model]
nhead = 8
nhid = 512
nlayers = 1
model_filepath = None
# model_type can be 'value' or 'policy'
model_type = 'value'

[train]
batch_size = 32
buffer_threshold = 32
positive_to_negative_ratio = 1
lr = 0.01
dropout = 0.1
n_required_validation_episodes = 2000
max_grad_norm = 0.5
#DQN:
epsilon = 0.1
gamma = 0.9
# mode can be 'positive_only' or 'balanced'
mode = 'balanced'
use_replay_buffer = False
num_buffers = 10000
fill_buffer_max_steps = 1000
batches_per_eval = 10
batches_per_train = 10
# scheduler
factor = 0.2
patience = 15
min_lr = 0.01