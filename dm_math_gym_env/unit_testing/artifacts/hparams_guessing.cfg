[run]
name = fill-buffer-1-step
seed = 42
gpu_id = 1
logging_batches_filename = 'log-batches.txt'
logging_q_values_filename = 'log-q-values.txt'

[env]
num_problems_per_module : 10 ** 6
validation_percentage : 0.2
max_sequence_length : 125
vocab_size : 250
max_difficulty : 0
univariate_differentiation : False
num_environments = 50
tokenizer_filepath = "artifacts/tokenizer.model"
max_formal_elements = 13
max_num_nodes = 10

data_dirpath = "artifacts"
selected_filenames = ["short_problems.txt"]

# used only to initialize the tokenizer
types = [
        "EquationOrExpression",
        "Equation",
        "Function",
        "Expression",
        "Variable",
        "Value",
        "Rational"
        ]

operators = [
            "lookup_value",
            "solve_system",
            "append",
            "append_to_empty_list",
            "factor",
            "differentiate",
            "mod",
            "gcd",
            "divides",
            "is_prime",
            "lcm",
            "lcd",
            "prime_factors",
            "evaluate_function",
            "not_op"
;            "differentiate_wrt",
;            "make_equation",
;            "simplify",
;            "make_function",
;            "replace_arg",
;            "lookup_value_equation",
;            "extract_isolated_variable",
;            "substitution_left_to_right",
            ]

[model]
nhead = 8
nhid = 512
nlayers = 2

model_filepath = None
# model_type can be 'value' or 'policy'
model_type = 'value'
action_embedding_size = 256
# Action Encoder
lstm_hidden_size = 1024
lstm_nlayers = 1

[train]
num_epochs = 1000000
replay_buffer_size = 30000
random_exploration_trajectory_cache_filepath = "mathematics_dataset-v1.0/uncomposed.sqlite"
model_exploration_trajectory_cache_filepath = "mathematics_dataset-v1.0/model_exploration_trajectory_cache.sqlite"

# fill buffer
fill_buffer_single_step_at_a_time = True
buffer_threshold = 10
positive_to_negative_ratio = 1
fill_buffer_mode = 'balanced'
num_batches_until_fill_buffer = 200
batches_per_fill_buffer = 5
max_num_attempts = 8000
reset_with_same_problem = False

# training
batch_size = 256
lr = 0.00005
max_grad_norm = 0.05
dropout = 0.1
weight_decay = 0.0001
epsilon = 0.4
gamma = 0.9
min_saved_steps_until_training = 10000
batches_per_epoch = 1

# LR scheduler
factor = 0.5
patience = 100000000000000000
min_lr = 0.00001
#max_lr = 0.01
#total_steps = 1000
#final_div_factor = 0.01

# replay priority
prioritization_exponent = 1
default_replay_buffer_priority = 1
n_batch_td_error = 2
sample_td_error_batch_size = 1024

# target network
use_target_network = False
batches_per_target_network_update = 500
batches_until_target_network = 5000000000000

# eval
n_required_validation_episodes = 200
batches_per_eval = 50

